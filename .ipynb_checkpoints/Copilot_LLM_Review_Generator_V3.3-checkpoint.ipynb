{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1e7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copilot LLM Review Generator V1.0\n",
    "# Created on: 23rd April, 2024\n",
    "# Modified on: 24th April, 2024\n",
    "# Author: Mu Sigma Inc.\n",
    "\n",
    "#This notebook has following features:\n",
    "    ##Generate a summary of the Copilot reviews from user prompt\n",
    "    ##Generate a comparison of Copilot features based on reviews from user prompt\n",
    "    ##Generate feature suggestion of Copilot based on the reviews from user prompt\n",
    "    ##Generate Quantitative numbers around Copilot Reviews from user prompt\n",
    "    ##Automatically identify the nature of the user question and what is being asked and print corresponding outputs\n",
    "    ##Retain context based on conversation history\n",
    "\n",
    "#In V2 version, we have made some bug fixes. We have also removed the retain context based on conversation history feature due to bugs.\n",
    "#In V3, we have improved the UI.\n",
    "#In V3.1, we have refined the prompts.\n",
    "#In V3.2, we have implemented exception handling\n",
    "#In V3.3, retaining context based on conversation history.\n",
    "\n",
    "\n",
    "#Import Required Libraries\n",
    "import gradio as gr\n",
    "import streamlit as st\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import openai\n",
    "import pyodbc\n",
    "import urllib\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import keyring\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from pandasai import SmartDataframe\n",
    "import pandas as pd\n",
    "from pandasai.llm import AzureOpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import base64\n",
    "import pandasql as ps\n",
    "from IPython.display import clear_output\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "#Initializing API Keys to use LLM\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"b71d4af1ea184bfb9444b448f4f5412a\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://fordmustang.openai.azure.com/\"\n",
    "\n",
    "\n",
    "#Reading the dataset\n",
    "Sentiment_Data  = pd.read_csv(\"Sampled_Copilot_Reviews_Final.csv\")\n",
    "\n",
    "#Function to derive Sentiment Score based on Sentiment\n",
    "def Sentiment_Score_Derivation(value):\n",
    "    try:\n",
    "        if value == \"positive\":\n",
    "            return 1\n",
    "        elif value == \"negative\":\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while deriving Sentiment Score: {e}\"\n",
    "        return err    \n",
    "\n",
    "#Deriving Sentiment Score and Review Count columns into the dataset\n",
    "Sentiment_Data[\"Sentiment_Score\"] = Sentiment_Data[\"Sentiment\"].apply(Sentiment_Score_Derivation)\n",
    "Sentiment_Data[\"Review_Count\"] = 1.0\n",
    "\n",
    "\n",
    "################################# Definiting Functions #################################\n",
    "\n",
    "#Review Summarization (Detailed) + Feature Comparison and Suggestion\n",
    "\n",
    "#Function to extract text from file\n",
    "def get_text_from_file(txt_file):\n",
    "    try:\n",
    "        with open(txt_file, 'r',encoding='latin') as file:\n",
    "            text = file.read()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while getting text from file: {e}\"\n",
    "        return err\n",
    "\n",
    "# Function to split text into chunks\n",
    "def get_text_chunks(text):\n",
    "    try:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while getting text chunks: {e}\"\n",
    "        return err\n",
    "\n",
    "# Function to create and store embeddings\n",
    "def get_vector_store(text_chunks):\n",
    "    try:\n",
    "        embeddings = AzureOpenAIEmbeddings(azure_deployment=\"MV_Agusta\")\n",
    "        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "        vector_store.save_local(\"faiss_index_CopilotSample\")\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while getting vectos: {e}\"\n",
    "        return err\n",
    "\n",
    "# Function to setup the vector store (to be run once or upon text update)\n",
    "def setup(txt_file_path):\n",
    "    try:\n",
    "        raw_text = get_text_from_file(txt_file_path)\n",
    "        text_chunks = get_text_chunks(raw_text)\n",
    "        get_vector_store(text_chunks)\n",
    "        print(\"Setup completed. Vector store is ready for queries.\")\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while setting up vector store: {e}\"\n",
    "        return err\n",
    "\n",
    "# Function to get conversational chain\n",
    "def get_conversational_chain_detailed(history):\n",
    "    try:\n",
    "        hist = \"\"\"\"\"\"\n",
    "        for i in history:\n",
    "            hist = hist+\"\\nUser: \"+i[0]\n",
    "            if isinstance(i[1],pd.DataFrame):\n",
    "                x = i[1].to_string()\n",
    "            else:\n",
    "                x = i[1]\n",
    "            hist = hist+\"\\nResponse: \"+x\n",
    "        prompt_template = \"\"\"\n",
    "         Given a dataset with these columns: Review, Data_Source, Geography, Product_Family, Sentiment and Aspect (also called Features)\n",
    "          \n",
    "          Review: This column contains the opinions and experiences of users regarding different product families across geographies, providing insights into customer satisfaction or complaints and areas for improvement.\n",
    "          Data_Source: This column indicates the platform from which the user reviews were collected, such as Reddit, Play Store, App Store, Tech Websites, or YouTube videos.\n",
    "          Geography: This column lists the countries of the users who provided the reviews, allowing for an analysis of regional preferences and perceptions of the products.\n",
    "          Product_Family: This column identifies the broader category of products to which the review pertains, enabling comparisons and trend analysis across different product families.\n",
    "          Sentiment: This column reflects the overall tone of the review, whether positive, negative, or neutral, and is crucial for gauging customer sentiment.\n",
    "          Aspect: This column highlights the particular features or attributes of the product that the review discusses, pinpointing areas of strength or concern.\n",
    "          \n",
    "          Perform the required task from the list below, as per user's query: \n",
    "          1. Review Summarization - Summarize the reviews by filtering the relevant Aspect, Geography, Product_Family, Sentiment or Data_Source, only based on available reviews and their sentiments in the dataset.\n",
    "          2. Aspect Comparison - Provide a summarized comparison for each overlapping feature/aspect between the product families or geographies ,  only based on available user reviews and their sentiments in the dataset. Include pointers for each aspect highlighting the key differences between the product families or geographies, along with the positive and negative sentiments as per customer perception.\n",
    "          3. New Feature Suggestion/Recommendation - Generate feature suggestions or improvements or recommendations based on the frequency and sentiment of reviews and mentioned aspects and keywords. Show detailed responses to user queries by analyzing review sentiment, specific aspects, and keywords.\n",
    "          4. Hypothetical Reviews - Generate positive and negative hypothetical reviews for any existing feature updatation or new feature addition in any device family across any geography, by simulating user reactions in different geographies based on existing data in the dataset. Ensure to synthesize realistic reviews that capture the sentiments and opinions of users, by considering their hypothetical prior experience working with the new feature.\n",
    "          \n",
    "          Enhance the model’s comprehension to accurately interpret user queries by:\n",
    "          Recognizing abbreviations for country names (e.g., ‘DE’ for Germany, ‘USA’or 'usa' or 'US' for the United States of America) and expanding them to their full names for clarity.\n",
    "          Understanding product family names even when written in reverse order or missing connecting words (e.g., ‘copilot in windows 11’ as ‘copilot windows’ and ‘copilot for security’ as ‘copilot security’ etc.).\n",
    "          Utilizing context and available data columns to infer the correct meaning and respond appropriately to user queries involving variations in product family names or geographical references\n",
    "          Please provide a comprehensive Review summary, feature comparison, feature suggestions for specific product families and actionable insights that can help in product development and marketing strategies.\n",
    "          Generate acurate response only, do not provide extra information.\n",
    "        \n",
    "          Important: Ensure to generate outputs using the provided dataset only, don't use pre-trained information to generate outputs.\n",
    "        \n",
    "          Context:\\n {context}?\\n\n",
    "          Question: \\n{question}\\n\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_deployment=\"Thruxton_R\",\n",
    "            api_version='2024-03-01-preview',\n",
    "            temperature = 0.4)\n",
    "        chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
    "        return chain\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while getting conversation chain for detailed review summarization: {e}\"\n",
    "        return err\n",
    "\n",
    "# Function to handle user queries using the existing vector store\n",
    "def query_detailed(user_question, history, vector_store_path=\"faiss_index_CopilotSample\"):\n",
    "    try:\n",
    "        embeddings = AzureOpenAIEmbeddings(azure_deployment=\"MV_Agusta\")\n",
    "        vector_store = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        chain = get_conversational_chain_detailed(history)\n",
    "        docs = vector_store.similarity_search(user_question)\n",
    "        response = chain({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\n",
    "        return response[\"output_text\"]\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while getting LLM response for detailed review summarization: {e}\"\n",
    "        return err\n",
    "\n",
    "\n",
    "## Review Summarization (Quantifiable)\n",
    "\n",
    "#Converting Top Operator to Limit Operator as pandasql doesn't support Top\n",
    "def convert_top_to_limit(sql):\n",
    "    try:\n",
    "        tokens = sql.upper().split()\n",
    "        is_top_used = False\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == 'TOP':\n",
    "                is_top_used = True\n",
    "                if i + 1 < len(tokens) and tokens[i + 1].isdigit():\n",
    "                    limit_value = tokens[i + 1]\n",
    "                    # Remove TOP and insert LIMIT and value at the end\n",
    "                    del tokens[i:i + 2]\n",
    "                    tokens.insert(len(tokens), 'LIMIT')\n",
    "                    tokens.insert(len(tokens), limit_value)\n",
    "                    break  # Exit loop after successful conversion\n",
    "                else:\n",
    "                    raise ValueError(\"TOP operator should be followed by a number\")\n",
    "\n",
    "        return ' '.join(tokens) if is_top_used else sql\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while converting Top to Limit in SQL Query: {e}\"\n",
    "        return err\n",
    "\n",
    "#Function to add Table Name into the SQL Query as it is, as the Table Name is Case Sensitive here\n",
    "def process_tablename(sql, table_name):\n",
    "    try:\n",
    "        x = sql.upper()\n",
    "        query = x.replace(table_name.upper(), table_name)\n",
    "        return query\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while processing table name in SQL query: {e}\"\n",
    "        return err\n",
    "\n",
    "## Generating Response by Identifying Prompt Nature\n",
    "\n",
    "#Function to get conversation chain for quantitative outputs and also add context from historical conversation as well\n",
    "def get_conversational_chain_quant(history):\n",
    "    try:\n",
    "        hist = \"\"\"\"\"\"\n",
    "        for i in history:\n",
    "            hist = hist+\"\\nUser: \"+i[0]\n",
    "            if isinstance(i[1],pd.DataFrame):\n",
    "                x = i[1].to_string()\n",
    "            else:\n",
    "                x = i[1]\n",
    "            hist = hist+\"\\nResponse: \"+x\n",
    "        prompt_template = \"\"\"\n",
    "        1. Your Job is to convert the user question to SQL Query (Follow Microsoft SQL server SSMS syntax.). You have to give the query so that it can be used on Microsoft SQL server SSMS.You have to only return query as a result.\n",
    "            2. There is only one table with table name Sentiment_Data where each row is a user review. The table has 10 columns, they are:\n",
    "                Review: Review of the Copilot Product\n",
    "                Data_Source: From where is the review taken. It contains following values: 'LaptopMag', 'PCMag', 'Verge', 'ZDNET', 'PlayStore', 'App Store','AppStore', 'Reddit', 'YouTube'.\n",
    "                Geography: From which Country or Region the review was given. It contains following values: 'Unknown', 'Brazil', 'Australia', 'Canada', 'China', 'Germany','France'.\n",
    "                Title: What is the title of the review\n",
    "                Review_Date: The date on which the review was posted\n",
    "                Product: Corresponding product for the review. It contains following values: 'COPILOT'.\n",
    "                Product_Family: Which version or type of the corresponding Product was the review posted for. It contains following values: 'Copilot in Windows 11', 'Copilot for Microsoft 365','Microsoft Copilot', 'Copilot for Security', 'Copilot Pro','Github Copilot', 'Copilot for Mobile'.\n",
    "                Sentiment: What is the sentiment of the review. It contains following values: 'positive', 'neutral', 'negative'.\n",
    "                Aspect: The review is talking about which aspect or feature of the product. It contains following values: 'Microsoft Product', 'Interface', 'Connectivity', 'Privacy','Compatibility', 'Generic', 'Innovation', 'Reliability','Productivity', 'Price', 'Text Summarization/Generation','Code Generation', 'Ease of Use', 'Performance','Personalization/Customization'.\n",
    "                Keyword: What are the keywords mentioned in the product\n",
    "                Review_Count - It will be 1 for each review or each row\n",
    "                Sentiment_Score - It will be 1, 0 or -1 based on the Sentiment.\n",
    "            3. Sentiment mark is calculated by sum of Sentiment_Score.\n",
    "            4. Net sentiment is calculcated by sum of Sentiment_Score divided by sum of Review_Count. It should be in percentage. Example:\n",
    "                    SELECT ((SUM(Sentiment_Score)*1.0)/(SUM(Review_Count)*1.0)) * 100 AS Net_Sentiment \n",
    "                    FROM Sentiment_Data\n",
    "                    ORDER BY Net_Sentiment DESC\n",
    "            5. Net sentiment across country or across region is sentiment mark of a country divided by total reviews of that country. It should be in percentage.\n",
    "                Example to calculate net sentiment across country:\n",
    "                    SELECT Geography, ((SUM(Sentiment_Score)*1.0) / (SUM(Review_Count)*1.0)) * 100 AS Net_Sentiment\n",
    "                    FROM Sentiment_Data\n",
    "                    GROUP BY Geography\n",
    "                    ORDER BY Net_Sentiment DESC\n",
    "            6. Net Sentiment across a column \"X\" is calculcated by Sentiment Mark for each \"X\" divided by Total Reviews for each \"X\".\n",
    "                Example to calculate net sentiment across a column \"X\":\n",
    "                    SELECT X, ((SUM(Sentiment_Score)*1.0) / (SUM(Review_Count)*1.0)) * 100 AS Net_Sentiment\n",
    "                    FROM Sentiment_Data\n",
    "                    GROUP BY X\n",
    "                    ORDER BY Net_Sentiment DESC\n",
    "            7. Distribution of sentiment is calculated by sum of Review_Count for each Sentiment divided by overall sum of Review_Count\n",
    "                Example: \n",
    "                    SELECT Sentiment, SUM(ReviewCount)*100/(SELECT SUM(Review_Count) AS Reviews FROM Sentiment_Data) AS Total_Reviews \n",
    "                    FROM Sentiment_Data \n",
    "                    GROUP BY Sentiment\n",
    "                    ORDER BY Total_Reviews DESC\n",
    "            8. Convert numerical outputs to float upto 1 decimal point.\n",
    "            9. Always include ORDER BY clause to sort the table based on the aggregate value calculated in the query.\n",
    "            10. Top Country is based on Sentiment_Score i.e., the Country which have highest sum(Sentiment_Score)\n",
    "            11. Always use 'LIKE' operator whenever they mention about any Country. Use 'LIMIT' operator instead of TOP operator.Do not use TOP OPERATOR. Follow syntax that can be used with pandasql.\n",
    "            12. If you are using any field in the aggregate function in select statement, make sure you add them in GROUP BY Clause.\n",
    "            13. Make sure to Give the result as the query so that it can be used on Microsoft SQL server SSMS.\n",
    "            14. Important: Always show Net_Sentiment in Percentage upto 1 decimal point. Hence always make use of ROUND function while giving out Net Sentiment and Add % Symbol after it.\n",
    "            15. Important: User can ask question about any categories including Aspects, Geograpgy, Sentiment etc etc. Hence, include the in SQL Query if someone ask it.\n",
    "            16. Important: You Response should directly starts from SQL query nothing else.\n",
    "            17. Important: Always use LIKE keyword instead of = symbol while generating SQL query.\n",
    "            18. Important: Generate outputs using the provided dataset only, don't use pre-trained information to generate outputs.\n",
    "        \\n Following is the previous conversation from User and Response, use it to get context only:\"\"\" + hist + \"\"\"\\n\n",
    "                Use the above conversation chain to gain context if the current prompt requires context from previous conversation.\\n\n",
    "        Context:\\n {context}?\\n\n",
    "        Question: \\n{question}\\n\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_deployment=\"Thruxton_R\",\n",
    "            api_version='2024-03-01-preview',\n",
    "            temperature = 0.6)\n",
    "        chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
    "        return chain\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while getting conversation chain for quantifiable review summarization: {e}\"\n",
    "        return err\n",
    "\n",
    "#Function to convert user prompt to quantitative outputs for Copilot Review Summarization\n",
    "def query_quant(user_question, history, vector_store_path=\"faiss_index_CopilotSample\"):\n",
    "    try:\n",
    "        # Initialize the embeddings model\n",
    "        embeddings = AzureOpenAIEmbeddings(azure_deployment=\"MV_Agusta\")\n",
    "        \n",
    "        # Load the vector store with the embeddings model\n",
    "        vector_store = FAISS.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        \n",
    "        # Rest of the function remains unchanged\n",
    "        chain = get_conversational_chain_quant(history)\n",
    "        docs = []\n",
    "        response = chain({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\n",
    "        SQL_Query = response[\"output_text\"]\n",
    "        SQL_Query = convert_top_to_limit(SQL_Query)\n",
    "        SQL_Query = process_tablename(SQL_Query,\"Sentiment_Data\")\n",
    "    #     print(SQL_Query)\n",
    "        data = ps.sqldf(SQL_Query, globals())\n",
    "        data_1 = data\n",
    "        html_table = data.to_html(index=False)\n",
    "    #     return html_table\n",
    "        return data_1\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while generating response for quantitative review summarization: {e}\"\n",
    "        return err\n",
    "\n",
    "\n",
    "## Generating Response by Identifying Prompt Nature\n",
    "\n",
    "\n",
    "#Function to identify the nature of prompt, whether the user is asking for a detailed summary or a quantitative summary\n",
    "def identify_prompt(user_question):\n",
    "    try:\n",
    "        prompt_template = \"\"\"\n",
    "        Given a user prompt about customer reviews for Product_Families(Copilot, Copilot for Microsoft 365, Copilot for Mobile, Copilot for Security, Copilot in Windows 11, Copilot Pro, Github Copilot, Microsoft Copilot) from Product Copilot and various different features, classify the prompt into one of two categories:\n",
    "            Quantifiable: This prompt seeks a numerical answer or data point related to the reviews. \n",
    "                            (e.g., What is the overall sentiment score for Product_Family X reviews?\n",
    "                                   How many reviews mention Feature Y in Product_Family Z?\n",
    "                                   Calculate the net sentiment score for reviews from Country A.\n",
    "                                   What is the sentiment distribution (positive, neutral, negative) for Product_Family X reviews?\n",
    "                                   How does the sentiment score vary across different sources (e.g., PlayStore, Reddit, YouTube) for Product_Family X?\n",
    "                                   Show the top N countries with the highest net sentiment scores for Product_Family X.\n",
    "                                   Provide a breakdown of sentiment scores by Product_Family.\n",
    "                                   What percentage of reviews mention Keyword K across all Product_Families?\n",
    "                                   Compare the net sentiment scores between Product_Family X and Product_Family Y.\n",
    "                                   How many reviews from Region Z have a sentiment score above a certain threshold for Product_Family X?\n",
    "            Detailed: This prompt seeks a summary, comparison, recommendation/suggestion or hypothetical reviews based on the reviews, expressed in words. The task can be either Review Summarization, Feature Comparison, New Feature Suggestion/Recommendation or Hypothetical Reviews generation based on the type of user question:\n",
    "                      Eg - Summarize / Give a summary of the reviews for different product families or geographies - Review Summarization \n",
    "                      Eg - Give feature comparison among product families a1,a2,a3... across geographies g1,g2,g3... or Compare the features of product families a1,a2,a3... across geographies g1,g2,g3,... or Compare utility of feature 'x' among product families a1,a2,a3... across geographies g1,g2,g3... - Aspect Comparison\n",
    "                      Eg - Suggest new features or improvements or recommendations for different product families in different geographies - New Feature Suggestion/Recommendation\n",
    "                      Eg - Generate hypothetical user reviews for the feature upgrade for any product family in any geography, focusing on the feature/aspect or Provide hypothetical user reviews for the addition of the new feature 'x' in any product family across any geography- Hypothetical Reviews\n",
    "\n",
    "        Input: User prompt about customer reviews\n",
    "        Output: Category (Quantifiable or Detailed)\n",
    "        Context:\\n {context}?\\n\n",
    "        Question: \\n{question}\\n\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_deployment=\"Thruxton_R\",\n",
    "            api_version='2024-03-01-preview')\n",
    "        chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
    "        response = chain({\"input_documents\": [], \"question\": user_question}, return_only_outputs=True)\n",
    "        if \"detailed\" in response[\"output_text\"].lower():\n",
    "            return \"Detailed\"\n",
    "        elif \"quantifiable\" in response[\"output_text\"].lower():\n",
    "            return \"Quantifiable\"\n",
    "        else:\n",
    "            return \"Others\"+\"\\nPrompt Identified as:\"+response[\"output_text\"]+\"\\n\"\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while generating conversation chain for identifying nature of prompt: {e}\"\n",
    "        return err\n",
    "\n",
    "#Function to generate Review Summarization (Detailed)/Feature Comparison/Feature Suggestion from User Prompt\n",
    "def review_summarization(user_question, history):\n",
    "    try:\n",
    "        txt_file_path = \"CopilotSamplewithAspect.txt\"\n",
    "        # Automatically call setup with the predefined file on startup\n",
    "        if not os.path.exists(\"faiss_index_CopilotSample\"):\n",
    "            setup(txt_file_path)\n",
    "\n",
    "        if os.path.exists(\"faiss_index_CopilotSample\"):\n",
    "            response = query_detailed(user_question, history)\n",
    "            return response\n",
    "        else:\n",
    "            return \"The vector store setup has failed. Please check the file path and try again.\"\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while generating detailed review summarization: {e}\"\n",
    "        return err\n",
    "\n",
    "#Function to generate Quantitative Review Summarization from User Prompt\n",
    "def quantifiable_data(user_question, history):\n",
    "    try:\n",
    "        response = query_quant(user_question, history)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while generating quantitative review summarization: {e}\"\n",
    "        return err\n",
    "\n",
    "#Function to generate a response from User Question\n",
    "def device_llm_review_generator(user_question, history):\n",
    "    try:\n",
    "        identity_prompt = identify_prompt(user_question)\n",
    "        if identity_prompt == \"Detailed\":\n",
    "            output = review_summarization(user_question, history)\n",
    "        elif identity_prompt == \"Quantifiable\":\n",
    "            output = quantifiable_data(user_question, history)\n",
    "        else:\n",
    "            output = \"Error: Cannot identify the nature of your question\\nPrompt identified as: \"+identity_prompt\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while generating LLM response: {e}\"\n",
    "        return err\n",
    "\n",
    "\n",
    "################################# Model Deployment #################################\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "    # Chat history state management\n",
    "        if 'chat_history' not in st.session_state:\n",
    "            st.session_state['chat_history'] = []\n",
    "\n",
    "        # Create a container for logos and title with horizontal layout\n",
    "        col1, col2, col3 = st.columns([1, 2, 1])\n",
    "      \n",
    "        # Display logo on the left\n",
    "        with col1:\n",
    "            st.image(\"microsoft_logo.png\", width=50)  # Adjust width as needed\n",
    "\n",
    "        # Display title in the center\n",
    "        with col2:\n",
    "            st.header(\"Copilot LLM Review Generator\")\n",
    "\n",
    "        # Display logo on the right\n",
    "        with col3:\n",
    "            st.image(\"copilot_logo.svg\", width=50)  # Align the logo to the right\n",
    "      \n",
    "        # User input section\n",
    "        user_input = st.text_input(\"Enter your text:\", placeholder=\"What would you like to process?\")\n",
    "\n",
    "        # Process button and output section\n",
    "        if st.button(\"Process\"):\n",
    "            output = device_llm_review_generator(user_input,st.session_state['chat_history'])\n",
    "            st.session_state['chat_history'].append((user_input, output))\n",
    "        \n",
    "            # Display output based on type (string or dataframe)\n",
    "            if isinstance(output, pd.DataFrame):\n",
    "                st.dataframe(output)\n",
    "            else:\n",
    "                st.write(output)\n",
    "\n",
    "        # Chat history section with some formatting\n",
    "        st.header(\"Chat History\")\n",
    "        for user_text, output_text in st.session_state['chat_history']:\n",
    "            st.markdown(f\"- You: {user_text}\")\n",
    "            if isinstance(output_text, pd.DataFrame):\n",
    "                st.dataframe(output_text)  # Convert dataframe to string for display\n",
    "            else:\n",
    "                st.markdown(f\"- Bot: {output_text}\")\n",
    "            st.write(\"---\")\n",
    "    except Exception as e:\n",
    "        err = f\"An error occurred while calling the final function: {e}\"\n",
    "        return err\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a25c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
